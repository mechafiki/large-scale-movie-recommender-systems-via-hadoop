{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Required installations"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["!pip install gdown\n","!pip install pyspark"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Required imports"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T16:53:27.092846Z","iopub.status.busy":"2023-05-27T16:53:27.092164Z","iopub.status.idle":"2023-05-27T16:53:27.100529Z","shell.execute_reply":"2023-05-27T16:53:27.099175Z","shell.execute_reply.started":"2023-05-27T16:53:27.092812Z"},"trusted":true},"outputs":[],"source":["from pyspark.sql import SparkSession\n","import pyspark.sql.functions as F\n","from keras.layers import Input,Dense\n","from keras.models import Model\n","from keras.optimizers import Adam\n","from keras.regularizers import l2\n","from sklearn.metrics import recall_score, precision_score,f1_score\n","import tensorflow as tf\n","from tensorflow.python.keras import backend as K\n","from tensorflow.keras.utils import get_custom_objects\n","get_custom_objects().update({'identity': tf.identity})\n","import numpy as np\n","import pandas as pd"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Spark Session\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T16:53:30.526528Z","iopub.status.busy":"2023-05-27T16:53:30.526155Z","iopub.status.idle":"2023-05-27T16:53:35.639942Z","shell.execute_reply":"2023-05-27T16:53:35.638659Z","shell.execute_reply.started":"2023-05-27T16:53:30.526500Z"},"trusted":true},"outputs":[],"source":["spark = SparkSession.builder \\\n","        .master(\"local[*]\") \\\n","        .appName(\"Recommender\") \\\n","        .config(\"spark.driver.memory\", \"16g\") \\\n","        .config(\"spark.executor.memory\", \"16g\") \\\n","        .getOrCreate()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Load and format the ratings data\n","\n","### 100k"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T12:36:46.921958Z","iopub.status.busy":"2023-05-27T12:36:46.921542Z","iopub.status.idle":"2023-05-27T12:37:02.414293Z","shell.execute_reply":"2023-05-27T12:37:02.413191Z","shell.execute_reply.started":"2023-05-27T12:36:46.921916Z"},"trusted":true},"outputs":[],"source":["!gdown 1lwPW7OefaJnwsaqYBQs-wgcIGiatYLXb\n","\n","def load_100k() :\n","    data = spark.read.option(\"delimiter\", \"\\t\")\\\n","                    .option(\"header\", \"False\")\\\n","                    .csv('/kaggle/working/u.data')\\\n","                    .select('_c0','_c1','_c2')\\\n","                    .withColumnRenamed('_c0','userId')\\\n","                    .withColumnRenamed('_c1', 'movieId') \\\n","                    .withColumnRenamed('_c2', 'rating')\n","    data = data.select([F.col(c).cast(\"int\") for c in data.columns])\n","    return data\n","\n","def dataPreprocessor(rating_df, movies_list, users_list):\n","\n","    df = pd.DataFrame(columns=users_list, index=movies_list).fillna(0)\n","    for (userID, itemID, rating) in rating_df.collect():\n","        df.loc[itemID,userID] = rating\n","\n","    return df.astype(np.float32)\n","\n","    \n","ratings_df = load_100k()\n","\n","movies_list = ratings_df.select(\"movieId\").distinct().rdd.flatMap(lambda x: x).collect()\n","users_list  = ratings_df.select(\"userId\").distinct().rdd.flatMap(lambda x: x).collect()\n","\n","num_users = len(users_list)\n","num_items = len(movies_list)\n","\n","train_df, test_df = ratings_df.sampleBy('movieId', fractions={val: 0.9 for val in ratings_df.select('movieId').distinct().rdd.map(lambda row: row[0]).collect()}, seed=42).randomSplit([0.9, 0.1], seed=42)\n","train_df, validation_df = train_df.sampleBy('movieId', fractions={val: 0.8 for val in train_df.select('movieId').distinct().rdd.map(lambda row: row[0]).collect()}, seed=42).randomSplit([0.8, 0.2], seed=42)\n","train_df      = dataPreprocessor(train_df,  movies_list, users_list)\n","validation_df = dataPreprocessor(validation_df,movies_list, users_list)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["###  1M"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T14:27:46.359276Z","iopub.status.busy":"2023-05-27T14:27:46.358885Z","iopub.status.idle":"2023-05-27T14:29:28.560768Z","shell.execute_reply":"2023-05-27T14:29:28.559627Z","shell.execute_reply.started":"2023-05-27T14:27:46.359245Z"},"trusted":true},"outputs":[],"source":["!gdown 18sHWE7Eu28hDqXib2PvesBYMea5AQmZs\n","\n","def load_1m() :\n","    data = spark.read.option(\"delimiter\", \"::\")\\\n","                    .option(\"header\", \"False\")\\\n","                    .csv('/kaggle/working/ratings.dat')\\\n","                    .select('_c0','_c1','_c2')\\\n","                    .withColumnRenamed('_c0','userId')\\\n","                    .withColumnRenamed('_c1', 'movieId') \\\n","                    .withColumnRenamed('_c2', 'rating')\n","    data = data.select([F.col(c).cast(\"int\") for c in data.columns])\n","    return data\n","    \n","def dataPreprocessor(rating_df, movies_list, users_list):\n","\n","    df = pd.DataFrame(columns=users_list, index=movies_list).fillna(0)\n","    for (userID, itemID, rating) in rating_df.collect():\n","        df.loc[itemID,userID] = rating\n","\n","    return df.astype(np.float32)\n","\n","    \n","ratings_df = load_1m()\n","\n","movies_list = ratings_df.select(\"movieId\").distinct().rdd.flatMap(lambda x: x).collect()\n","users_list  = ratings_df.select(\"userId\").distinct().rdd.flatMap(lambda x: x).collect()\n","\n","num_users = len(users_list)\n","num_items = len(movies_list)\n","\n","train_df, test_df = ratings_df.sampleBy('movieId', fractions={val: 0.9 for val in ratings_df.select('movieId').distinct().rdd.map(lambda row: row[0]).collect()}, seed=42).randomSplit([0.9, 0.1], seed=42)\n","train_df, validation_df = train_df.sampleBy('movieId', fractions={val: 0.9 for val in train_df.select('movieId').distinct().rdd.map(lambda row: row[0]).collect()}, seed=42).randomSplit([0.9, 0.1], seed=42)\n","train_df      = dataPreprocessor(train_df,  movies_list, users_list)\n","validation_df = dataPreprocessor(validation_df,movies_list, users_list)                                 "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### 10M"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T16:53:55.913295Z","iopub.status.busy":"2023-05-27T16:53:55.912905Z","iopub.status.idle":"2023-05-27T17:11:41.746532Z","shell.execute_reply":"2023-05-27T17:11:41.745193Z","shell.execute_reply.started":"2023-05-27T16:53:55.913258Z"},"trusted":true},"outputs":[],"source":["!gdown 1e064MFX83PYtPDcISjYQw4fTQtv-PG38\n","\n","def load_10m() :\n","    data = spark.read.option(\"delimiter\", \"::\")\\\n","                    .option(\"header\", \"False\")\\\n","                    .csv('/kaggle/working/ratings.dat')\\\n","                    .select('_c0','_c1','_c2')\\\n","                    .withColumnRenamed('_c0','userId')\\\n","                    .withColumnRenamed('_c1', 'movieId') \\\n","                    .withColumnRenamed('_c2', 'rating')\n","    data = data.select([F.col(c).cast(\"int\") for c in data.columns])\n","    return data\n","    \n","def dataPreprocessor(rating_df, movies_list, users_list):\n","\n","    df = pd.DataFrame(columns=users_list, index=movies_list).fillna(0)\n","    for (userID, itemID, rating) in rating_df.collect():\n","        df.loc[itemID,userID] = rating\n","\n","    return df.astype(np.float32)\n","\n","    \n","ratings_df = load_10m()\n","\n","movies_list = ratings_df.select(\"movieId\").distinct().rdd.flatMap(lambda x: x).collect()\n","users_list  = ratings_df.select(\"userId\").distinct().rdd.flatMap(lambda x: x).collect()\n","\n","num_users = len(users_list)\n","num_items = len(movies_list)\n","\n","train_df, test_df = ratings_df.sampleBy('movieId', fractions={val: 0.9 for val in ratings_df.select('movieId').distinct().rdd.map(lambda row: row[0]).collect()}, seed=42).randomSplit([0.9, 0.1], seed=42)\n","train_df      = dataPreprocessor(train_df,  movies_list, users_list)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T17:11:41.751938Z","iopub.status.busy":"2023-05-27T17:11:41.750568Z"},"scrolled":true,"trusted":true},"outputs":[],"source":["def masked_mse(y_true, y_pred):\n","    # masked function\n","    mask_true = K.cast(K.not_equal(y_true, 0), K.floatx())\n","    # masked squared error\n","    masked_squared_error = K.square(mask_true * (y_true - y_pred))\n","    masked_mse = K.sum(masked_squared_error) / K.maximum(K.sum(mask_true), 1)\n","    return masked_mse\n","\n","def masked_rmse_clip(y_true, y_pred):\n","    # masked function\n","    mask_true = K.cast(K.not_equal(y_true, 0), K.floatx())\n","    # masked squared error\n","    masked_squared_error = K.square(mask_true * (y_true - y_pred))\n","    masked_rmse = K.sqrt(K.sum(masked_squared_error) / K.maximum(K.sum(mask_true), 1))\n","    return masked_rmse\n","\n","def I_AutoRec(n, k, f, g, reg, lr):\n","    \"\"\"\n","    IAutoRec is an item-based AutoRec model.\n","\n","    Args:\n","        n: The number of items.\n","        k: The number of hidden units.\n","        f: The activation function for the hidden layer.\n","        g: The activation function for the output layer.\n","        reg: The regularization strength (used to prevent overfitting).\n","        lr: The learning rate for the optimizer.\n","\n","    Returns:\n","        An I_AutoRec model.\n","    \"\"\"\n","\n","    # Define the input layer\n","    input_layer = Input(shape=(n,))\n","    \n","    # Define the encoding layer\n","    encoded = Dense(k, activation=g, kernel_regularizer=l2(reg), use_bias=True)(input_layer)\n","    \n","    # Define the decoding layer\n","    decoded = Dense(n, activation=f, kernel_regularizer=l2(reg), use_bias=True)(encoded)\n","\n","    # Create the model with input and output layers\n","    model = Model(input_layer, decoded)\n","    \n","    # Compile the model with masked MSE loss and masked RMSE metric\n","    model.compile(optimizer=Adam(learning_rate=lr), loss=masked_mse, metrics=[masked_rmse_clip])\n","\n","    return model\n","\n","# Create an instance of I_AutoRec model\n","autorec = I_AutoRec(\n","    n=num_users,\n","    k=1000,  # 500 for 100k - 500 for 1M\n","    g='sigmoid',\n","    f='identity',\n","    reg=0.001,\n","    lr=0.0001\n","    )\n","\n","# Train the model\n","history = autorec.fit(\n","    x=train_df,\n","    y=train_df,\n","#     validation_data=[train_df,validation_df],\n","    epochs=200,\n","    batch_size=512)   #128 for 100k - 256 for 1M -\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-05-27T14:43:53.622645Z","iopub.status.busy":"2023-05-27T14:43:53.622212Z","iopub.status.idle":"2023-05-27T14:44:14.489649Z","shell.execute_reply":"2023-05-27T14:44:14.488384Z","shell.execute_reply.started":"2023-05-27T14:43:53.622594Z"},"trusted":true},"outputs":[],"source":["def create_binarised_output(ratings):\n","    binary = []\n","    for rating in ratings:\n","        if rating > treshold:\n","            binary.append(1)\n","        else:\n","            binary.append(0)\n","    return binary\n","\n","treshold = 3.5\n","\n","true =  dataPreprocessor(test_df,  movies_list, users_list).values.flatten()\n","pred = np.array(autorec.predict(train_df), dtype=np.float32).flatten()\n","rmse = masked_rmse_clip(true, pred)\n","\n","print(f\"RMSE1: {rmse}\")\n","\n","nonzero_indices = np.nonzero(true)[0]\n","\n","y = true[nonzero_indices].tolist()\n","pred = pred[nonzero_indices].tolist()\n","\n","y_binary = create_binarised_output(y)\n","pred_binary = create_binarised_output(pred)\n","\n","precision = precision_score(y_binary, pred_binary)\n","print(\"Precision:\", precision)\n","\n","# Calculate recall\n","recall = recall_score(y_binary, pred_binary)\n","print(\"Recall:\", recall)\n","\n","# Calculate accuracy\n","accuracy = f1_score(y_binary, pred_binary)\n","print(\"Accuracy:\", accuracy)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Making recommendations for a given user"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":true},"outputs":[],"source":["# # Get predictions for test set\n","# predictions = autorec.predict(np.array(test_df, dtype=np.float32))\n","# predictions = K.clip(predictions, 1, 5)\n","# predictions = pd.DataFrame(predictions, index=users_list,columns=movies_list)\n","\n","# # Get the user ID or index in the test data\n","# user_id = 943\n","\n","# # Extract the row of the user's ratings from the test matrix\n","# user_ratings = test_df.loc[:,user_id]\n","\n","# # Get the indices where the user has rated the item\n","# rated_item_indices = user_ratings[user_ratings != 0].index\n","\n","# not_rated_item_indices = list( set(movies_list)-set(rated_item_indices))\n","\n","# # Get all predicted user ratings\n","# predicted_user_ratings = predictions.loc[user_id , not_rated_item_indices]\n","\n","# sorted_ratings = predicted_user_ratings.sort_values(ascending=False)\n","\n","# recommendations = sorted_ratings.index[:20]\n","# print(\"Recommendations for user\", user_id, \":\")\n","# for i, movie_id in enumerate(recommendations):\n","#     print(i+1, \".\", movie_id , \"  \\t\\t predicted rating : \",sorted_ratings.values[i] )\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
